{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb0c7df7-6586-4797-be19-d9db9162e932",
   "metadata": {},
   "source": [
    "# Recommender systems\n",
    "\n",
    "__Recommender systems__ are software algorithms or techniques that provide personalized recommendations to users, suggesting items or content that they are likely to find interesting, useful, or relevant. These systems are widely used in various industries, including e-commerce, entertainment, social media, and content streaming platforms.\n",
    "\n",
    "The concept behind recommender systems is based on the idea of leveraging user preferences, historical data, and patterns to predict and suggest __items__ that __a user__ might enjoy or benefit from. By analyzing user behavior, such as past purchases, ratings, search history, and interactions with items or content, recommender systems aim to understand user preferences and make accurate recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729cb21-aba7-4872-bf1a-c70bf6d22280",
   "metadata": {},
   "source": [
    "We can use large language models (LLMs) as recommender systems. E.g. you may ask ChatGPT about what movie to watch based on what movies you like or ask for a music that are similar to the provided example.\n",
    "\n",
    "You can use the following prompts:\n",
    "\n",
    "```\n",
    "I want you to act as a movie recommender system. I will provide you with a list of movies that I like, and your goal is to recommend 10 other movies that I will also enjoy.\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "I want you to act as a song recommender. I will provide you with a song and you will create a playlist of 10 songs that are similar to the given song. And you will provide a playlist name and description for the playlist. Do not choose songs that are same name or artist. Do not write any explanations or other words, just reply with the playlist name, description and the songs. My first song is \"...\".\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362167f-c8e4-494e-910f-f54f5ea5efa9",
   "metadata": {},
   "source": [
    "However, language models like ChatGPT typically don't have a built-in knowledge base. Therefore, they may recommend items that don't actually exist.\n",
    "\n",
    "Furthermore, most recommender systems are designed for specific business domains where the items are not publicly available.\n",
    "\n",
    "If you have a small number of items, you can directly provide them to ChatGPT for better recommendations. For example, you can use the following prompt:\n",
    "\n",
    "```\n",
    "I want you to act as an online shop recommender system. My shop has ten products with the following descriptions:\n",
    "\n",
    "Product: Wireless Bluetooth Earbuds\n",
    "Description: Enjoy immersive sound with these wireless Bluetooth earbuds. They provide crystal-clear audio and a comfortable fit, perfect for music lovers on the go.\n",
    "\n",
    "Product: Portable Power Bank\n",
    "Description: Stay charged wherever you are with this portable power bank. It offers high-capacity charging, multiple ports, and a compact design, making it ideal for travel or emergencies.\n",
    "\n",
    "Product: Smart Fitness Tracker\n",
    "Description: Keep track of your fitness goals with this smart fitness tracker. It monitors your heart rate, steps, sleep patterns, and more, while providing real-time notifications and a sleek design.\n",
    "\n",
    "Product: Programmable Robot Toy\n",
    "Description: Spark your child's imagination with this programmable robot toy. It teaches coding concepts through play, enabling kids to explore STEM subjects in a fun and interactive way.\n",
    "\n",
    "Product: Waterproof DSLR Camera Bag\n",
    "Description: Protect your valuable camera gear with this waterproof DSLR camera bag. It features padded compartments, adjustable dividers, and a durable exterior, ensuring your equipment stays safe and dry.\n",
    "\n",
    "Product: Electric Air Fryer\n",
    "Description: Cook healthier meals with this electric air fryer. It uses hot air circulation to fry food with little to no oil, resulting in crispy and delicious dishes while reducing fat intake.\n",
    "\n",
    "Product: Portable Bluetooth Speaker\n",
    "Description: Take your music anywhere with this portable Bluetooth speaker. It delivers high-quality sound, has a long battery life, and is water-resistant, making it perfect for outdoor adventures.\n",
    "\n",
    "Product: Wireless Charging Pad\n",
    "Description: Simplify charging with this wireless charging pad. Compatible with Qi-enabled devices, it allows you to power up your phone or other devices by simply placing them on the pad.\n",
    "\n",
    "Product: Virtual Reality Headset\n",
    "Description: Immerse yourself in virtual worlds with this virtual reality headset. It offers a 360-degree panoramic view, adjustable straps for comfort, and compatibility with most smartphones.\n",
    "\n",
    "Product: Instant Pot Pressure Cooker\n",
    "Description: Save time in the kitchen with this versatile Instant Pot pressure cooker. It combines multiple functions like pressure cooking, slow cooking, sautéing, and more, making meal preparation a breeze.\n",
    "\n",
    "Your goal is to recommend items from this list based on the previous purchases. I will provide you with list of purchases for specific user and you will give me the product name.\n",
    "```\n",
    "\n",
    "Due to the limited size of the LLM's memory, we cannot pass a large number of items. Therefore, we still need to train different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f6a5cd-be30-40d3-a2f2-b5ff3a49cf39",
   "metadata": {},
   "source": [
    "Let's consider the Movielens-1m dataset. It is a benchmark dataset used for recommender systems and machine learning research. It consists of approximately 1 million ratings from 6,000 users on movies. The dataset includes user demographics, movie titles, and genre information. It's widely used for evaluating recommendation algorithms and collaborative filtering techniques.\n",
    "\n",
    "Here are the main components of the MovieLens-1M dataset:\n",
    "\n",
    "1. _Ratings_: The dataset includes approximately 1 million movie ratings provided by around 6,000 users. Each rating is represented by a triplet consisting of a user ID, a movie ID, and a rating value on a scale of 1 to 5. These ratings were collected from the MovieLens website between 2000 and 2003.\n",
    "\n",
    "2. _Users_: The dataset provides demographic information about the users, such as age, gender, occupation, and ZIP code. User IDs are anonymized to preserve privacy.\n",
    "\n",
    "3. _Movies_: The dataset contains details about the movies, including their titles, genres, and release year. Each movie is identified by a unique movie ID.\n",
    "\n",
    "4. _Genre Information_: MovieLens-1M includes a list of predefined genres, and each movie is associated with one or more genres from this set. The genre information allows researchers to explore the relationship between user preferences and movie genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d62da-95de-407b-ab8f-2e4f773b9208",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-18 16:03:55--  https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
      "Распознаётся files.grouplens.org (files.grouplens.org)… 128.101.65.152\n",
      "Подключение к files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 5917549 (5,6M) [application/zip]\n",
      "Сохранение в: «ml-1m.zip.1»\n",
      "\n",
      "ml-1m.zip.1         100%[===================>]   5,64M   553KB/s    за 17s     \n",
      "\n",
      "2023-05-18 16:04:22 (349 KB/s) - «ml-1m.zip.1» сохранён [5917549/5917549]\n",
      "\n",
      "Archive:  ml-1m.zip\n",
      "replace ml-1m/movies.dat? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "! wget https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "! unzip ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a23c6ba-3577-4fee-8080-c7d45bf1b1b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings:\n",
      "   user_id  movie_id  rating  timestamp\n",
      "0        1      1193       5  978300760\n",
      "1        1       661       3  978302109\n",
      "2        1       914       3  978301968\n",
      "3        1      3408       4  978300275\n",
      "4        1      2355       5  978824291\n",
      "\n",
      "Users:\n",
      "   user_id gender  age  occupation zipcode\n",
      "0        1      F    1          10   48067\n",
      "1        2      M   56          16   70072\n",
      "2        3      M   25          15   55117\n",
      "3        4      M   45           7   02460\n",
      "4        5      M   25          20   55455\n",
      "\n",
      "Movies:\n",
      "   movie_id                               title                        genres\n",
      "0         1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2         3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3         4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4         5  Father of the Bride Part II (1995)                        Comedy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load ratings data\n",
    "ratings_file = 'ml-1m/ratings.dat'\n",
    "ratings_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv(ratings_file, sep='::', engine='python', names=ratings_cols, encoding='latin-1')\n",
    "\n",
    "# Load users data\n",
    "users_file = 'ml-1m/users.dat'\n",
    "users_cols = ['user_id', 'gender', 'age', 'occupation', 'zipcode']\n",
    "users = pd.read_csv(users_file, sep='::', engine='python', names=users_cols, encoding='latin-1')\n",
    "\n",
    "# Load movies data\n",
    "movies_file = 'ml-1m/movies.dat'\n",
    "movies_cols = ['movie_id', 'title', 'genres']\n",
    "movies = pd.read_csv(movies_file, sep='::', engine='python', names=movies_cols, encoding='latin-1')\n",
    "\n",
    "# Example usage: Print the first few rows of each dataframe\n",
    "print(\"Ratings:\")\n",
    "print(ratings.head())\n",
    "\n",
    "print(\"\\nUsers:\")\n",
    "print(users.head())\n",
    "\n",
    "print(\"\\nMovies:\")\n",
    "print(movies.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b670038b-a0af-4b2d-bfa9-9634c9f03a1f",
   "metadata": {},
   "source": [
    "Before we train any model, it is important to properly define the train and validation subsamples. The choice of splitting strategy depends on your business and recommendation process. In recommender systems, we usually follow two types of train/test split:\n",
    "\n",
    "1. __Split by user/items__: This strategy involves organizing the data in a way that all observations for a specific user or item are either in the train or test set. This approach is useful for evaluating how well the model performs in a cold-start scenario. For instance, when dealing with an app where user profiles change frequently, and there is limited historical data, you still want to provide recommendations based on user characteristics. This method is typically applied in content-based or hybrid models.\n",
    "\n",
    "2. __Temporal Split__: When the data has a temporal aspect, such as user ratings or interactions over time, it is crucial to consider the temporal order of the data. In this case, you can split the data into training and testing sets using a cutoff point. All interactions that occur before the cutoff are used for training, while interactions that happen after the cutoff are used for testing. This ensures that the model is evaluated on the most recent user-item interactions.\n",
    "\n",
    "Let us follow the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6d6b3f-1b8d-4319-8dd2-8b30e6f0e0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings_sorted = ratings.sort_values('timestamp')\n",
    "\n",
    "# Calculate the cutoff timestamp for the temporal split\n",
    "cutoff_ratio = 0.8  # 80% of data for training\n",
    "cutoff_index = int(len(ratings_sorted) * cutoff_ratio)\n",
    "cutoff_timestamp = ratings_sorted.iloc[cutoff_index]['timestamp']\n",
    "\n",
    "# Split ratings into training and testing sets based on the cutoff timestamp\n",
    "train_ratings = ratings_sorted[ratings_sorted['timestamp'] < cutoff_timestamp]\n",
    "test_ratings = ratings_sorted[ratings_sorted['timestamp'] >= cutoff_timestamp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b382036-30e5-4fde-8468-6469b46a1deb",
   "metadata": {},
   "source": [
    "There are generally three main types of recommender systems:\n",
    "\n",
    "1. __Content-Based Filtering__: This approach focuses on the attributes or characteristics of items and users. It analyzes the features and properties of items (e.g., product descriptions, genres, or tags) and user profiles (e.g., user demographics, preferences, or historical interactions) to recommend items that match a user's preferences. Content-based filtering is particularly useful when there is limited or no historical data available about user preferences.\n",
    "\n",
    "2. __Collaborative Filtering__: This approach is based on the assumption that users with similar preferences in the past will have similar preferences in the future. Collaborative filtering methods analyze the behavior of a large group of users to find patterns and make recommendations. It can be further divided into two subtypes:\n",
    "\n",
    "    1. __User__-Based Collaborative Filtering: It recommends items to a user based on the interests and preferences of users who are similar to them.\n",
    "\n",
    "    2. __Item__-Based Collaborative Filtering: It recommends items that are similar to the ones a user has already interacted with or shown interest in.\n",
    "\n",
    "3. __Hybrid Methods__: These methods combine collaborative filtering and content-based filtering to take advantage of their respective strengths. By leveraging both user behavior and item attributes, hybrid recommender systems can provide more accurate and diverse recommendations.\n",
    "\n",
    "\n",
    "The common factor in these methods is __similarity__. Therefore, we need a way to measure the similarity between users or items. For example, we can encode features of users and items to find similar ones. This is an example of content-based filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9708774a-3ed3-4616-81ed-414a04fdc13d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "user_features = users[['gender', 'age']].values\n",
    "user_features = np.hstack([pd.get_dummies(users['occupation']).values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739cb68-17e4-455e-83fe-6a329171318f",
   "metadata": {},
   "source": [
    "To find the closests users we can utilize nearest neighbors search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "73a28292-8be3-4908-a42e-684958d7dd6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 6040/6040 [00:38<00:00, 158.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import trange\n",
    "\n",
    "# Create a Nearest Neighbors model\n",
    "k = 3  # Number of nearest neighbors to consider\n",
    "nn_model = NearestNeighbors(n_neighbors=k, metric=\"cosine\")\n",
    "nn_model.fit(user_features)\n",
    "\n",
    "def user_based_knn_preds(ratings, users, user_features, nn_model, all_items):\n",
    "    predictions = []\n",
    "    for target_user_index in trange(users.shape[0]):\n",
    "        target_user_features = user_features[target_user_index].reshape(1, -1)\n",
    "        # Find the nearest neighbors of the target user\n",
    "        distances, indices = nn_model.kneighbors(target_user_features)\n",
    "        \n",
    "        # Get the user IDs of the nearest neighbors\n",
    "        nearest_user_ids = users.iloc[indices[1:, 0]]['user_id']\n",
    "        target_rated_items = ratings[ratings['user_id'] == users.iloc[target_user_index].user_id]['movie_id'].values\n",
    "\n",
    "        # Filter ratings data for the nearest users\n",
    "        user_ratings = ratings[ratings['user_id'].isin(nearest_user_ids)]\n",
    "        rated_items = user_ratings['movie_id'].values\n",
    "        unrated_items = np.setdiff1d(rated_items, target_rated_items)\n",
    "        # Calculate average ratings of unrated items\n",
    "        avg_ratings = user_ratings.groupby('movie_id')['rating'].mean().reset_index()\n",
    "\n",
    "        # Sort unrated items by average rating in descending order\n",
    "        sorted_items = avg_ratings[avg_ratings['movie_id'].isin(unrated_items)]\n",
    "        sorted_items = sorted_items.sort_values('rating', ascending=False)\n",
    "        # Get the top recommended items\n",
    "        top_items = sorted_items.head(20)  # Change the number of recommendations as desired\n",
    "        preds = top_items.movie_id.tolist()\n",
    "        if len(preds) < 20:\n",
    "            preds.extend(np.random.choice(list(all_items - set(rated_items)), size=10-len(preds)))\n",
    "        predictions.append(preds)\n",
    "    return np.array(predictions)\n",
    "\n",
    "all_items = set(ratings.movie_id)\n",
    "predicted_ratings = user_based_knn_preds(train_ratings, users, user_features, nn_model, all_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2dca06-afb1-4f8a-93b3-a22c5366e039",
   "metadata": {},
   "source": [
    "Metrics for recommender systems can be splitted into two groups:\n",
    "1. Offline Metrics:\n",
    "\n",
    "    * Offline metrics are used to evaluate recommender systems based on historical data without considering real-time user interactions or business impact.\n",
    "    * These metrics help to understand the performance of the model during the training and validation phases, providing insights into its effectiveness in learning from the data.\n",
    "    * Examples of offline metrics include precision, recall, F1 score, mean average precision (MAP), mean absolute error (MAE), root mean squared error (RMSE), and others.\n",
    "    * These metrics assess the accuracy, coverage, ranking quality, and prediction errors of the recommender system based on the historical data.\n",
    "    * Offline metrics are useful for comparing different models, tuning hyperparameters, and assessing the general performance of the recommender system.\n",
    "2. Online Metrics:\n",
    "\n",
    "    * Online metrics are focused on evaluating the performance of recommender systems in real-world scenarios and measure their impact on user engagement and business goals.\n",
    "    * These metrics consider real-time user interactions, such as clicks, purchases, likes, dislikes, time spent in the app, conversion rates, revenue generated, and other user engagement signals.\n",
    "    * Online metrics are more directly aligned with business objectives and provide insights into how the recommender system influences user behavior and generates value.\n",
    "    * Examples of online metrics include click-through rate (CTR), conversion rate, revenue per user, engagement rate, retention rate, and others.\n",
    "    * Online metrics enable organizations to assess the effectiveness of their recommender systems in driving user engagement, user satisfaction, and business outcomes.\n",
    "\n",
    "While offline metrics are valuable during the development and testing phases of a recommender system, online metrics provide a more comprehensive evaluation of the system's performance in real-world usage. It's important to consider both types of metrics to understand the strengths and limitations of the recommender system and make informed decisions regarding model selection, algorithm improvements, and business strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5646c8-fe20-4c0a-b9b2-912c027124a3",
   "metadata": {},
   "source": [
    "Let us implement the __HitRate@K__. It measures the proportion of users for whom at least one relevant item is present in the top-K recommendations. It focuses on whether the recommended items contain at least one relevant item for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "61d4c74e-cb5c-4d6c-8456-88fe402995b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HitRate@20: 0.22\n"
     ]
    }
   ],
   "source": [
    "def calculate_hit_rate_at_k(test_ratings, predicted_ratings):\n",
    "    hits = 0\n",
    "    for user_id, user_ratings in test_ratings.groupby('user_id'):\n",
    "        actual_ratings = set(user_ratings['movie_id'])\n",
    "        top_K_predicted = predicted_ratings[users[users.user_id == user_id].index[0]]\n",
    "\n",
    "        if any(movie_id in actual_ratings for movie_id in top_K_predicted):\n",
    "            hits += 1\n",
    "\n",
    "    hit_rate_at_K = hits / len(test_ratings['user_id'].unique())\n",
    "    return hit_rate_at_K\n",
    "\n",
    "print(f\"HitRate@20: {calculate_hit_rate_at_k(test_ratings, predicted_ratings):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460780d-7421-4c7b-9acd-3ed6bc09c5f4",
   "metadata": {},
   "source": [
    "__Mean Average Precision at K (MAP@K)__ is an evaluation metric commonly used in information retrieval tasks, including recommender systems. It measures the average precision at different recall levels within the top-K recommendations.\n",
    "\n",
    "Here's a step-by-step explanation of MAP@K:\n",
    "\n",
    "1. Precision:\n",
    "\n",
    "    * Precision is a metric that quantifies the accuracy of the recommendations. It calculates the proportion of relevant items among the recommended items at a given position.\n",
    "    * Precision is computed as the number of relevant items found in the recommendations divided by the total number of items recommended at that position.\n",
    "2. Average Precision:\n",
    "\n",
    "    * Average Precision (AP) extends the concept of precision by considering multiple positions within the recommendation list.\n",
    "    * For each user, AP is calculated by summing the precisions at relevant positions and dividing it by the total number of relevant items.\n",
    "    * AP ranges from 0 to 1, with higher values indicating better performance.\n",
    "3. Mean Average Precision at K:\n",
    "\n",
    "    * MAP@K is the average of the AP values across all users in the evaluation set.\n",
    "    * It provides a single score that represents the average performance of the recommender system in terms of precision at various recall levels within the top-K recommendations.\n",
    "\n",
    "To calculate MAP@K, follow these steps:\n",
    "\n",
    "1. For each user in the evaluation set:\n",
    "\n",
    "    * Determine the set of relevant items based on the ground truth or user feedback.\n",
    "    * Retrieve the top-K recommendations generated by the recommender system.\n",
    "2. Calculate the precision at each position up to K for each user:\n",
    "\n",
    "    * Check if the recommended item at each position is relevant.\n",
    "    * If it is, calculate the precision by dividing the number of relevant items found so far by the position.\n",
    "3. Calculate the average precision (AP) for each user:\n",
    "    * Sum the precisions at relevant positions and divide by the total number of relevant items.\n",
    "4. Compute the mean of the average precision (AP) values across all users to obtain MAP@K.\n",
    "\n",
    "MAP@K provides a measure of the quality of the ranking order within the top-K recommendations. It considers both the relevance of the items and their positions, providing a comprehensive evaluation of the recommender system's performance in terms of precision at different recall levels.\n",
    "\n",
    "__Note__: The value of K in MAP@K represents the cutoff position within the recommendation list. It determines the length of the list to consider when calculating the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4030986a-f72d-4f6e-88c6-34388f880a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@10: 0.30\n"
     ]
    }
   ],
   "source": [
    "def calculate_map_at_k(test_ratings, predicted_ratings):\n",
    "    average_precisions = []\n",
    "    for user_id, user_ratings in test_ratings.groupby('user_id'):\n",
    "        actual_ratings = set(user_ratings['movie_id'])\n",
    "        top_K_predicted = predicted_ratings[users[users.user_id == user_id].index[0]]\n",
    "\n",
    "        precision = 0\n",
    "        num_correct = 0\n",
    "\n",
    "        for j, movie_id in enumerate(top_K_predicted):\n",
    "            if movie_id in actual_ratings:\n",
    "                num_correct += 1\n",
    "                precision += num_correct / (j + 1)\n",
    "\n",
    "        if num_correct > 0:\n",
    "            average_precision = precision / num_correct\n",
    "            average_precisions.append(average_precision)\n",
    "    return np.mean(average_precisions)\n",
    "\n",
    "print(f\"MAP@10: {calculate_map_at_k(test_ratings, predicted_ratings):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9423af24-0502-4654-94c3-d92809fff73d",
   "metadata": {},
   "source": [
    "Looks like simple similarity between user features yields some results. However, such an approach does not account for user behavior.\n",
    "\n",
    "To incorporate user behavior, we need to encode interactions between users and items. The common practice is to utilize a user-item interaction matrix. This matrix captures user interactions with items and represents them in a matrix format, where users are rows and items are columns. By utilizing this matrix, we can capture user behavior and preferences, enabling personalized recommendations.\n",
    "\n",
    "The user-item interaction matrix serves as a foundation for various recommendation algorithms and facilitates efficient processing and analysis of user-item interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dd13fcdc-70a1-44a9-bbf9-9f0c71380746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the user-item interaction matrix\n",
    "interaction_matrix = train_ratings.pivot(index='user_id', columns='movie_id', values='rating')\n",
    "\n",
    "interaction_matrix = interaction_matrix.reindex(movies.movie_id.tolist(), axis=1)\n",
    "interaction_matrix = interaction_matrix.reindex(users.user_id.values)\n",
    "\n",
    "# Fill missing values with 0 (indicating no interaction)\n",
    "interaction_matrix = interaction_matrix.fillna(0)\n",
    "\n",
    "# Convert the interaction matrix to a NumPy array if needed\n",
    "interaction_matrix = interaction_matrix.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34943891-e2fa-4fce-902c-6f27aa75d351",
   "metadata": {},
   "source": [
    "Let us use this matrix as vectors for similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b749c570-69ef-4550-acd6-15eedab1f122",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6040/6040 [09:49<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HitRate@10: 0.22\n",
      "MAP@10: 0.31\n"
     ]
    }
   ],
   "source": [
    "# Create a Nearest Neighbors model\n",
    "k = 3  # Number of nearest neighbors to consider\n",
    "nn_model = NearestNeighbors(n_neighbors=k, metric=\"cosine\")\n",
    "nn_model.fit(interaction_matrix)\n",
    "\n",
    "predicted_ratings = user_based_knn_preds(train_ratings, users, interaction_matrix, nn_model, all_items)\n",
    "print(f\"HitRate@10: {calculate_hit_rate_at_k(test_ratings, predicted_ratings):.2f}\")\n",
    "print(f\"MAP@10: {calculate_map_at_k(test_ratings, predicted_ratings):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06101b7-c2b3-424f-913f-c089ff88b4ae",
   "metadata": {},
   "source": [
    "The interaction matrix plays a crucial role in improving the performance of recommendations. However, these vectors are often sparse and high-dimensional, which can lead to poor performance when using nearest neighbor (NN) search due to the curse of dimensionality.\n",
    "\n",
    "To overcome this issue, models with latent factors or embeddings are commonly used, with matrix factorization being a popular approach. By decomposing the interaction matrix into lower-dimensional latent factors or embeddings, these models effectively reduce sparsity and address the curse of dimensionality. They capture the underlying relationships between users and items, resulting in better recommendations compared to simple nearest neighbor methods.\n",
    "\n",
    "Matrix factorization and latent factor models help overcome the challenges posed by sparse and high-dimensional user-item interaction matrices. They provide a more accurate and efficient way to model user preferences and item characteristics, enhancing the performance of recommender systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fab576a5-0158-4206-93f7-558f82a2f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "# Convert interaction matrix to sparse form\n",
    "sparse_interactions = sp.csr_array(interaction_matrix)\n",
    "\n",
    "# Perform matrix factorization using SVD\n",
    "user_factors, _, item_factors = sp.linalg.svds(sparse_interactions, k=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a19e0-92b3-48c6-bb7d-74891246eaef",
   "metadata": {},
   "source": [
    "We can similarly utilize nearest neighbors search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2efaaa62-3b6b-4436-899c-6b7656b30dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 6040/6040 [00:36<00:00, 165.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HitRate@10: 0.23\n",
      "MAP@10: 0.33\n"
     ]
    }
   ],
   "source": [
    "# Create a Nearest Neighbors model\n",
    "k = 3  # Number of nearest neighbors to consider\n",
    "nn_model = NearestNeighbors(n_neighbors=k, metric=\"cosine\")\n",
    "nn_model.fit(user_factors)\n",
    "\n",
    "predicted_ratings = user_based_knn_preds(train_ratings, users, user_factors, nn_model, all_items)\n",
    "print(f\"HitRate@10: {calculate_hit_rate_at_k(test_ratings, predicted_ratings):.2f}\")\n",
    "print(f\"MAP@10: {calculate_map_at_k(test_ratings, predicted_ratings):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5530ee-d439-41e7-a523-63c06fb8a2b3",
   "metadata": {},
   "source": [
    "Or directly predict ratings in initial matrix using dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fe195937-2059-4bec-bd24-a00b1a483a70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HitRate@10: 0.42\n",
      "MAP@10: 0.25\n",
      "CPU times: user 3.03 s, sys: 253 ms, total: 3.28 s\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted_ratings = user_factors.dot(item_factors)\n",
    "predicted_ratings = np.argsort(predicted_ratings)[:, -20:]\n",
    "\n",
    "print(f\"HitRate@10: {calculate_hit_rate_at_k(test_ratings, predicted_ratings):.2f}\")\n",
    "print(f\"MAP@10: {calculate_map_at_k(test_ratings, predicted_ratings):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa93d4-34f0-4d05-9ac8-4ac599db74e6",
   "metadata": {},
   "source": [
    "We also can use latent representations to find similar items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0b678d0-7094-4e32-b038-a5020a3ca325",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because you watched Toy Story (1995):\n",
      "\t Babe (1995)\n",
      "\t Toy Story 2 (1999)\n",
      "\t Aladdin (1992)\n",
      "\t Groundhog Day (1993)\n",
      "\t Babe: Pig in the City (1998)\n",
      "\t Mighty Peking Man (Hsing hsing wang) (1977)\n",
      "\t Wrong Trousers, The (1993)\n",
      "\t Bug's Life, A (1998)\n",
      "\t Grand Day Out, A (1992)\n"
     ]
    }
   ],
   "source": [
    "# Create a Nearest Neighbors model\n",
    "k = 10  # Number of nearest neighbors to consider\n",
    "nn_model = NearestNeighbors(n_neighbors=k, metric=\"cosine\")\n",
    "nn_model.fit((item_factors / (item_factors ** 2).sum(axis=0)[np.newaxis, :]).T)\n",
    "\n",
    "def find_similar_movies(title, item_factors, nn_model, movies):\n",
    "    idx = movies[movies.title == title].index\n",
    "    _, preds = nn_model.kneighbors(item_factors[idx])\n",
    "    print(f\"Because you watched {title}:\")\n",
    "    for i in preds[0][1:]:\n",
    "        print(\"\\t\", movies.iloc[i].title)\n",
    "\n",
    "find_similar_movies(\"Toy Story (1995)\", (item_factors / (item_factors ** 2).sum(axis=0)[np.newaxis, :]).T, nn_model, movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0fea2-0566-4d03-b44b-83f523311d1c",
   "metadata": {},
   "source": [
    "This vectors do not about film genres and preserves only the information about user-item interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d65f95-f708-4c2e-98c5-6443a2e3403b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
